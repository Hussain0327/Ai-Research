# Configuration for optimizer ablation study
# Compares different optimizers and learning rates

model:
  vocab_size: 50257
  d_model: 128
  n_layers: 6
  n_heads: 8
  max_seq_len: 256
  dropout: 0.1

data:
  dataset_name: "tinystories"
  tokenizer_type: "char"
  max_length: 256
  stride: 128
  batch_size: 32
  num_workers: 4
  max_samples: 5000
  data_fraction: 0.05
  seed: 42

training:
  num_epochs: 3
  learning_rate: 3e-4  # Will be swept: [1e-4, 3e-4, 1e-3]
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  use_amp: true
  eval_interval: 150
  save_interval: 400
  log_interval: 50
  optimizer: "adamw"  # Will be swept: ["adamw", "adam", "sgd"]

experiment:
  name: "optimizer_ablation"
  description: "Optimizer and learning rate comparison"
  tags: ["ablation", "optimizer", "learning_rate"]

# Sweep parameters
sweep:
  parameter: "learning_rate"
  values: [1e-4, 3e-4, 1e-3]
  secondary_parameter: "optimizer"
  secondary_values: ["adamw", "adam"]