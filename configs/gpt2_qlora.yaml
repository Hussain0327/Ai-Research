model_name: gpt2
output_dir: checkpoints/gpt2_qlora_demo

data:
  train_file: data/sample/train.txt
  val_file: null
  block_size: 128

lora:
  r: 8
  alpha: 16
  dropout: 0.05

train:
  epochs: 1
  batch_size: 2
  lr: 1.0e-4
  gradient_accumulation: 1

