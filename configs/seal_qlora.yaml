model_name: gpt2

seal:
  inner_steps: [1, 3, 10, 30]
  lora_rank: [4, 8]
  lr: 5.0e-4

data:
  train_file: data/sample/train.txt
  val_file: data/sample/train.txt
  block_size: 128

io:
  baseline_adapter: checkpoints/gpt2_qlora_demo
  save_dir: checkpoints/gpt2_seal/adapt
  results: results/gpt2_seal/adapt.jsonl

