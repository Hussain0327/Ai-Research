# Base configuration for TinyGPT experiments
# This serves as the default configuration template

model:
  vocab_size: 50257  # Will be updated based on tokenizer
  d_model: 128
  n_layers: 6
  n_heads: 8
  max_seq_len: 256
  dropout: 0.1

data:
  dataset_name: "tinystories"
  tokenizer_type: "char"  # "char" or "subword"
  max_length: 256
  stride: 128
  batch_size: 32
  num_workers: 4
  max_samples: 10000  # Limit for faster experiments
  data_fraction: 0.1  # Use 10% of dataset
  seed: 42

training:
  num_epochs: 3
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0
  use_amp: true
  grad_accum_steps: 1
  eval_interval: 200
  save_interval: 500
  log_interval: 50

# Experiment tracking
experiment:
  name: "base_experiment"
  description: "Base TinyGPT experiment"
  tags: ["baseline", "char_tokenizer"]
