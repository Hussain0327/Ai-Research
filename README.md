# TinyGPT – Scaling Law Research Harness

[![CI/CD Pipeline](https://github.com/Hussain0327/Ai-Research/actions/workflows/ci.yml/badge.svg)](https://github.com/Hussain0327/Ai-Research/actions/workflows/ci.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)

TinyGPT is a reproducible research sandbox for measuring how lightweight GPT-style
language models respond to changes in model scale, data budget, and optimization
strategy. The codebase keeps the architecture intentionally compact so that you
can run scaling-law studies on laptops (Apple Silicon, CUDA laptops) while still
capturing the metrics needed to fit the classic "loss ∝ N^α" curves.

---

## Research agenda

- **Scaling behaviour** – quantify how validation loss, perplexity, and accuracy
  change as we vary hidden size (`d_model`), depth, heads, context length, and
  gradient-accumulation steps.
- **Data efficiency** – down-sample TinyStories or ingest custom corpora to study
  how loss curves shift with reduced data availability (`data_fraction`,
  `max_samples`).
- **Optimizer/regularisation sweeps** – compare Adam/AdamW/SGD plus weight decay
  and grad clipping, and record throughput statistics (`tokens_per_sec`).
- **Research data packaging** – every run emits machine-readable `results.json`,
  checkpoint metadata, and consolidated sweep summaries that downstream scripts
  turn into scaling law plots and CSVs.

---

## Research data & experimental assets

| Asset | Source / Location | What it contains | Typical use |
|-------|-------------------|------------------|-------------|
| `roneneldan/TinyStories` | Hugging Face datasets | 200k+ short juvenile narratives. Acts as the default pretraining corpus. | Baseline scaling experiments and tokenizer benchmarking. |
| Custom text files (set `data.dataset_name: "custom"`) | Local `.txt` corpora | Plain-text loader with configurable train/val splits. | Ablations on vocabulary size, domain shift experiments. |
| `checkpoints/<run>/` | Generated locally | Checkpoints (`step_N.pt`), `config.yaml`, and `results.json` with final metrics. | Reproducibility; feeding into evaluation / export scripts. |
| `experiments/<sweep>/sweep_summary.json` | Generated by `scripts/run_sweep.py` | Aggregated metrics for each configuration / seed combination. | Power-law fitting, plotting with `scripts/analyze_scaling.py`. |
| `results/evaluation_results.csv` | Generated by `src/eval.py` | Metrics across checkpoints (loss, perplexity, accuracy, parameter counts). | Scaling-law fitting and reporting. |

Each `results.json` produced by `Trainer.train()` looks like:

```json
{
  "final_val_loss": 3.42,
  "final_val_perplexity": 30.6,
  "best_val_loss": 3.18,
  "total_steps": 1800,
  "model_parameters": 1459200
}
```

These files – combined with the configs stored next to them – are the primary
research artefacts used by the analysis scripts.

---

## Repository map

```
src/
├── data/
│   ├── datamodule.py          # TinyStories + simple text data loaders
│   └── tokenizers.py          # Character & subword tokenizers + collate
├── models/
│   └── tiny_gpt.py            # Minimal GPT block (pre-norm, tied embeddings)
├── train.py                   # Trainer, CLI, checkpointing, metrics
├── eval.py                    # Scaling-aware evaluator and plotting helpers
└── utils/                     # Misc utilities used by training scripts

scripts/
├── run_sweep.py               # Parameter sweeps & sweep summaries
├── analyze_scaling.py         # Power-law fitting + plots from sweep outputs
└── export_model.py            # Bundle checkpoints with tokenizer artefacts

configs/                       # YAML defaults for experiments & sweeps
tests/                         # Pytest suite covering data, model, trainer, scripts
Makefile                       # Convenience targets (lint, test, format)
pyproject.toml                 # Dependency + tooling configuration
```

---

## Environment setup

```bash
git clone https://github.com/Hussain0327/Ai-Research.git
cd Ai-Research

python3 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip

# Core + dev dependencies (PyTorch CPU/MPS by default)
pip install -e ".[dev]"

# (Optional) install CUDA wheels for torch if you have an NVIDIA GPU
# pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

python - <<'PY'
import torch
print("torch", torch.__version__, "CUDA:", torch.cuda.is_available(), "MPS:", torch.backends.mps.is_available())
PY
```

The project targets Python 3.10+, but pytest is configured for 3.8+ to ease CI.

---

## Running a single experiment

1. Pick a config (see `configs/base_config.yaml` for reference).
2. Decide where to write checkpoints/results (defaults to `checkpoints/`).
3. Launch training:

```bash
python -m src.train \
  --config configs/base_config.yaml \
  --save_dir checkpoints/base_run \
  --seed 42
```

During training the trainer logs:

- `train_loss`, `learning_rate`, and `tokens_per_sec` every `log_interval` steps.
- Validation loss/perplexity every `eval_interval` steps.
- Checkpoints every `save_interval` steps plus the best/last model.

You will find the research artefacts for the run under `checkpoints/base_run/`:

- `step_<N>.pt` – serialized model, optimizer, scheduler, config, and metrics.
- `best_model.pt` / `final_model.pt` – convenience copies for evaluation.
- `training_results.json` – same payload as the snippet in the data table.

---

## Benchmark snapshots (before/after training)

Record the performance of a model **prior to** training (fresh weights) and
**after** convergence so you can quantify improvements or regressions. A simple
workflow is:

1. Export the initial checkpoint (or instantiate the model and run `src.eval`
   with `--max_batches` set low). Save the resulting `evaluation_results.csv`
   as `results/<run>/baseline.csv`.
2. Train the model and rerun `src.eval` on the best checkpoint to produce
   `results/<run>/post_training.csv`.
3. Append the key metrics to the table below.

| Run / Config | Stage | Val Loss | Val Perplexity | Notes |
|--------------|-------|----------|----------------|-------|
| `sample_custom` | Baseline (0 epochs) | 3.5752 | 35.70 | From `results/sample_custom/baseline.csv` (random weights) |
| `sample_custom` | After training (10 epochs) | 3.0959 | 22.11 | From `results/sample_custom/post_training.csv`; best loss 3.0788 |

> Tip: the raw CSVs live in `results/sample_custom/`. Update or append rows when
> you repeat the experiment with new seeds or configs.
> The GPU run above also produced `results/sample_custom_eval/evaluation_results.csv`
> and `scaling_laws.png` for richer analysis.

---

## Collecting research data from sweeps

`scripts/run_sweep.py` deep-copies the base config per experiment and records a
`sweep_summary.json` with one entry per configuration / seed. Example command:

```bash
python scripts/run_sweep.py \
  --config configs/ablation_optimizer.yaml \
  --output_dir experiments/optimizer_ablation \
  --seeds 42 43 44
```

Each experiment folder contains:

- `config.yaml` – the exact parameters used.
- `results.json` – metrics for the converged model.
- `training_results.json` – (optional) if you ran via `src.train` CLI.

`experiments/optimizer_ablation/sweep_summary.json` aggregates all runs and feeds
directly into `scripts/analyze_scaling.py`.

---

## Analyzing scaling laws & evaluation

### Evaluate checkpoints

```bash
python -m src.eval \
  --config configs/base_config.yaml \
  --checkpoint_dir checkpoints/base_run \
  --output_dir results/base_run_eval \
  --max_batches 100
```

Outputs:

- `results/base_run_eval/evaluation_results.csv`
- `results/base_run_eval/scaling_laws.json` and `scaling_laws.png`
- Optional text generations if `--generate_samples` is passed.

### Fit scaling curves from many runs

```bash
python scripts/analyze_scaling.py \
  --checkpoint_dirs experiments/optimizer_ablation \
  --output_dir analysis/optimizer_ablation
```

You can pass multiple directories via a comma-separated list in
`--checkpoint_dirs` if your sweeps are stored in different folders. The analysis
script consumes either `sweep_summary.json` files or individual `results.json`
scattered under each checkpoint directory, then produces:

- Log/log scatter plots per scaling dimension (width, depth, context length,
  data fraction, parameter count).
- Fitted equations (power-law or log-power) with R² reported to the console.
- Residual plots to assess fit quality.

These artefacts capture the empirical "research data" you need to report scaling
laws or compare architectures.

---

## Export for downstream use

To package a trained checkpoint together with the tokenizer artefacts:

```bash
python scripts/export_model.py \
  --checkpoint_dir checkpoints/base_run \
  --out_dir exports/tiny_gpt_base \
  --tokenizer_type char
```

The export script copies the best checkpoint, saves the tokenizer vocabulary,
and records metadata (parameter count, dataset details) for deployment or
downstream evaluation.

---

## Quality checks

| Purpose | Command |
|---------|---------|
| Format (Black + isort) | `black .` & `isort .` |
| Lint (flake8, mypy) | `flake8 src tests` & `mypy src` |
| Tests (fast subset) | `pytest -q` |
| Coverage snapshot | `pytest --cov=src --cov-report=term-missing` |

All of the above are wired into the CI workflow badge at the top of this file.

---

## Contributing & roadmap

Pull requests are welcome. Current areas that would improve the research story:

1. Add more public corpora (OpenWebText, The Pile slices) to broaden scaling
   ranges beyond TinyStories.
2. Implement additional optimizers (Adafactor, Lion) or smoothing techniques for
   better small-batch performance.
3. Extend evaluation with text quality metrics (distinct-n, KL divergence against
   reference corpora) to complement perplexity curves.

See `CONTRIBUTING.md` for coding standards and pre-commit hooks.

---

## License

MIT – see [LICENSE](LICENSE).

If you build upon these experiments in academic work, please cite the repository
URL so others can reproduce your scaling-law setup.
